{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWFPoxlqEjze"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python - <<'PY'\n",
    "import random, numpy as np, torch, os\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "print(\"Seeds fixed to\", SEED)\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbpJN35cVYAI"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.52.2 accelerate==1.7.0 datasets==2.20.0 sqlalchemy==2.0.30 sqlite-utils==3.38 evaluate==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1281,
     "status": "ok",
     "timestamp": 1747857607352,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "MqwJNvPSju4U",
    "outputId": "8ba1904b-adad-4bd7-946d-3487c19b4704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate                            1.7.0\n",
      "datasets                              2.20.0\n",
      "evaluate                              0.4.3\n",
      "sentence-transformers                 4.1.0\n",
      "sqlite-utils                          3.38\n",
      "tensorflow-datasets                   4.9.8\n",
      "transformers                          4.52.2\n",
      "vega-datasets                         0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E 'transformers|accelerate|datasets|sqlalchemy|sqlite-utils|moz-sql-parser|evaluate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggRRHKJJc9RI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XEw0G_9oE5C"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "spider_train   = load_dataset(\"spider\")[\"train\"]\n",
    "spider_val = load_dataset(\"spider\")[\"validation\"]\n",
    "\n",
    "print(spider_train)\n",
    "print(spider_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRPywpplHiJC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501,
     "referenced_widgets": [
      "27b965a7348b49a8b8ad7273b3d1f7d4",
      "9687eabbe5514da8bc475fa5ed4e1d0e",
      "f2d210323c5e44fb8d3fb44f2c44afe5",
      "2716660f92254ef6abe90b7cfe8c4157",
      "1e43713ebfa2455d867b1487875ab5ff",
      "37c40d810f9142949f7cee8d14747dce",
      "aa7e83b290484038b31f6accf73ec714",
      "db8757e19b0249e1ad5279b0e2ccb254",
      "30d169b1e3e144c39b3b453c46323e3d",
      "ea35617346ab41309034158a72463f73",
      "f6438de878544783934be004023e10d1"
     ]
    },
    "executionInfo": {
     "elapsed": 12360,
     "status": "ok",
     "timestamp": 1747857627254,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "SZmtUIQfonbc",
    "outputId": "d8a6f825-c361-4b51-c043-16239ac97364"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b965a7348b49a8b8ad7273b3d1f7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"  # use preferred model\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "model      = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Sj1mmVtD43n"
   },
   "outputs": [],
   "source": [
    "!pip install -q kagglehub            # tiny, pure-python package\n",
    "import kagglehub, os, json, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Either:\n",
    "#   kagglehub.login()                              # will prompt\n",
    "#   – or –\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"\"        # <- set once per session\n",
    "os.environ[\"KAGGLE_KEY\"]     = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6140,
     "status": "ok",
     "timestamp": 1747857635939,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "CaJayzRfBA1n",
    "outputId": "7ec00416-4554-4598-e969-950433141310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No spider.zip – using unpacked folder\n",
      "Spider databases live at: /root/.cache/kagglehub/datasets/jeromeblanchet/yale-universitys-spider-10-nlp-dataset/versions/1/spider/database\n"
     ]
    }
   ],
   "source": [
    "import kagglehub, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path( kagglehub.dataset_download(\n",
    "           \"jeromeblanchet/yale-universitys-spider-10-nlp-dataset\") )\n",
    "\n",
    "# 1️⃣  Look for a zip first (old mirror versions)\n",
    "zip_files = list(root.rglob(\"spider.zip\"))\n",
    "if zip_files:\n",
    "    print(\"Found spider.zip – extracting…\")\n",
    "    with zipfile.ZipFile(zip_files[0], \"r\") as zf:\n",
    "        zf.extractall(\"spider\")          # creates ./spider/database/…\n",
    "    SPIDER_DIR = Path(\"spider\")\n",
    "\n",
    "# 2️⃣  Otherwise assume the folder is already there (current mirror)\n",
    "else:\n",
    "    print(\"No spider.zip – using unpacked folder\")\n",
    "    # the mirror root itself is usually “…/spider/”, but we search just in case\n",
    "    try:\n",
    "        SPIDER_DIR = next(root.rglob(\"spider/database\")).parent\n",
    "    except StopIteration as e:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not locate Spider data inside the mirror download\") from e\n",
    "\n",
    "DB_ROOT = SPIDER_DIR / \"database\"\n",
    "print(\"Spider databases live at:\", DB_ROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_eNsZV8ObYy"
   },
   "outputs": [],
   "source": [
    "DB_ROOT = Path(\"/kaggle/input/yale-universitys-spider-10-nlp-dataset/spider/database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-9xQnq7O17o"
   },
   "outputs": [],
   "source": [
    "#   reflect every .sqlite file to create a plain-text schema description\n",
    "\n",
    "\n",
    "import sqlalchemy as sa\n",
    "\n",
    "_schema_cache: dict[str, str] = {}            # {db_id: \"…schema string…\"}\n",
    "\n",
    "def get_schema_string(db_id: str) -> str:\n",
    "    \"\"\"Return a compact textual schema for the given Spider database.\"\"\"\n",
    "    if db_id in _schema_cache:\n",
    "        return _schema_cache[db_id]\n",
    "\n",
    "    db_file = DB_ROOT / db_id / f\"{db_id}.sqlite\"\n",
    "    engine  = sa.create_engine(f\"sqlite:///{db_file}\")\n",
    "    insp    = sa.inspect(engine)\n",
    "\n",
    "    parts   = []\n",
    "    for tbl in sorted(insp.get_table_names()):\n",
    "        cols = [c[\"name\"] for c in insp.get_columns(tbl)]\n",
    "        parts.append(f\"{tbl}({', '.join(cols)})\")\n",
    "\n",
    "    schema_str = \", \".join(parts)\n",
    "    _schema_cache[db_id] = schema_str\n",
    "    return schema_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPJpeyHYL0g3"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Use any split you like – here we sample 5 from Spider-train\n",
    "NUM_SHOTS      = 5\n",
    "spider_train   = load_dataset(\"spider\")[\"train\"]\n",
    "\n",
    "random.seed(42\n",
    "            )\n",
    "# Turn each row into a normal dict and attach the schema string\n",
    "DEMO_SET = [\n",
    "    {\n",
    "        **ex,                                  # copy all original fields\n",
    "        \"schema_str\": get_schema_string(ex[\"db_id\"])\n",
    "    }\n",
    "    for ex in spider_train.shuffle(seed=42).select(range(NUM_SHOTS))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IopUzH9BZyk"
   },
   "outputs": [],
   "source": [
    "def build_prompt(nl_question: str, target_schema: str) -> str:\n",
    "    \"\"\"\n",
    "    Compose a 5-shot prompt:\n",
    "        [global instruction]\n",
    "        For i = 1..5:   ### Example-i Schema: …   ### Example-i Question: …   ### Example-i SQL: …\n",
    "        ### Database Schema:  [schema for *this* db]\n",
    "        ### Question:         [new NL question]\n",
    "        ### SQL:\n",
    "    \"\"\"\n",
    "    parts = [\n",
    "        \"### Instruction:\\n\"\n",
    "        \"You are an expert SQL developer. Given a database schema and a natural-language\\n\"\n",
    "        \"question, write ONE syntactically correct SQL query that answers the question.\\n\"\n",
    "        \"Return **only** the SQL; do not repeat the schema or add explanations.\\n\"\n",
    "    ]\n",
    "\n",
    "    for i, ex in enumerate(DEMO_SET, 1):\n",
    "        parts.append(f\"### Example {i} Schema:\\n{ex['schema_str']}\\n\")\n",
    "        parts.append(f\"### Example {i} Question:\\n{ex['question']}\\n\")\n",
    "        parts.append(f\"### Example {i} SQL:\\n{ex['query'].strip()}\\n\")\n",
    "\n",
    "    parts.append(f\"### Database Schema:\\n{target_schema}\\n\")\n",
    "    parts.append(f\"### Question:\\n{nl_question}\\n\")\n",
    "    parts.append(\"### SQL:\\n\")\n",
    "\n",
    "    return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9NgOWu2Br84"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def generate_sql(nl_question: str, schema: str) -> str:\n",
    "    prompt     = build_prompt(nl_question, schema)\n",
    "    inputs     = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_len  = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    out        = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    gen_tokens = out[0, input_len:]\n",
    "    sql        = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # trim markdown fences etc.\n",
    "    #sql = sql.split(\"```\")[-1].strip()\n",
    "    sql = sql.split(\";\")[0].replace(\"\\n\", \" \").strip()\n",
    "    #return sql\n",
    "\n",
    "    # keep everything *before* first sentinel\n",
    "    sql = re.split(r\"(###|\\n\\n)\", sql, maxsplit=1)[0]\n",
    "    # remove prefix for llama3.1\n",
    "    prefix = \"```sql \"\n",
    "    if sql.startswith(prefix):\n",
    "      sql = sql[len(prefix):]\n",
    "    else:\n",
    "      sql = sql\n",
    "\n",
    "\n",
    "    return sql\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2244,
     "status": "ok",
     "timestamp": 1747837155597,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "vRGHQP2-DAhE",
    "outputId": "454f121f-b324-47f9-9cbe-325faade224a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold SQL:  SELECT count(*) FROM singer\n",
      "Predicted SQL:  SELECT COUNT(DISTINCT T1.Singer_ID) FROM singer AS T1 JOIN singer_in_concert AS T2 ON T1.Singer_ID = T2.Singer_ID\n"
     ]
    }
   ],
   "source": [
    "example      = spider_val[1]\n",
    "db_id        = example[\"db_id\"]\n",
    "schema_str   = get_schema_string(db_id)\n",
    "predicted_sql= generate_sql(example[\"question\"], schema_str)\n",
    "print(\"Gold SQL: \", example[\"query\"])\n",
    "print(\"Predicted SQL: \", predicted_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKS8_46KoSi-"
   },
   "outputs": [],
   "source": [
    "def exact_match(pred, gold):\n",
    "    return pred.strip().lower() == gold.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1O-8Fw5MxE8w"
   },
   "outputs": [],
   "source": [
    "from sqlglot import parse_one, expressions\n",
    "\n",
    "def flatten_ast(node):\n",
    "    \"\"\"\n",
    "    Recursively collect all node‐type names and literal values as lowercase strings.\n",
    "    \"\"\"\n",
    "    out = set()\n",
    "\n",
    "    def walk(n):\n",
    "        # record the AST node type\n",
    "        out.add(type(n).__name__.lower())\n",
    "\n",
    "        # record any literal (e.g. identifiers, strings, numbers)\n",
    "        if hasattr(n, \"this\") and isinstance(n.this, (str, int, float)):\n",
    "            out.add(str(n.this).lower())\n",
    "\n",
    "        # recurse into child expressions\n",
    "        for arg in n.args.values():\n",
    "            if isinstance(arg, list):\n",
    "                for child in arg:\n",
    "                    if isinstance(child, expressions.Expression):\n",
    "                        walk(child)\n",
    "            elif isinstance(arg, expressions.Expression):\n",
    "                walk(arg)\n",
    "\n",
    "    walk(node)\n",
    "    return out\n",
    "\n",
    "def component_match(pred_sql, gold_sql):\n",
    "    try:\n",
    "        pred_ast = parse_one(pred_sql)\n",
    "        gold_ast = parse_one(gold_sql)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "    pred_set = flatten_ast(pred_ast)\n",
    "    gold_set = flatten_ast(gold_ast)\n",
    "    if not gold_set:\n",
    "        return 0.0\n",
    "    return len(pred_set & gold_set) / len(gold_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QXKGCbe6KZz"
   },
   "outputs": [],
   "source": [
    "import sqlite3, pandas as pd, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_query(sql: str, db_path: Path):\n",
    "    \"\"\"Return query result as a sorted list of tuples (order-independent).\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            df = pd.read_sql_query(sql, conn)\n",
    "        # sort rows + cols for order-invariant comparison\n",
    "        return tuple(map(tuple, df.sort_index(axis=1).sort_values(list(df.columns)).to_numpy()))\n",
    "    except Exception as e:\n",
    "        # any failure counts as wrong\n",
    "        return f\"ERROR-{e}\"\n",
    "\n",
    "def execution_accuracy(dataset):\n",
    "    \"\"\"Compute Spider-style Execution Accuracy on a HF split (e.g. validation).\"\"\"\n",
    "    correct = 0\n",
    "    for ex in tqdm(dataset, desc=\"Evaluating\"):\n",
    "        db_id   = ex[\"db_id\"]\n",
    "        schema  = get_schema_string(db_id)\n",
    "        pred_sql= generate_sql(ex[\"question\"], schema)\n",
    "\n",
    "        db_file = DB_ROOT / db_id / f\"{db_id}.sqlite\"\n",
    "        gold    = run_query(ex[\"query\"],     db_file)\n",
    "        pred    = run_query(pred_sql,        db_file)\n",
    "\n",
    "        if gold == pred:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1747837177009,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "h0LnNiUx1vPx",
    "outputId": "ff5bb339-756a-4020-964f-163792e23cdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['db_id', 'query', 'question', 'query_toks', 'query_toks_no_value', 'question_toks'],\n",
       "    num_rows: 1034\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1265,
     "status": "ok",
     "timestamp": 1747837179122,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "fPEFqKOLwCpE",
    "outputId": "7067221d-568b-49af-def2-67ba75fbce42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT count(DISTINCT Nationality) FROM conductor\n",
      "SELECT COUNT(DISTINCT T1.Nationality) FROM conductor AS T1\n"
     ]
    }
   ],
   "source": [
    "p = 835\n",
    "print(spider_val[p]['query'])\n",
    "print(generate_sql(spider_val[p]['question'], get_schema_string(spider_val[p]['db_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1235,
     "status": "ok",
     "timestamp": 1747837181739,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "Vap2MOwMRHfU",
    "outputId": "9f296b85-8e42-4c07-a62e-15199c036950"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match(generate_sql(spider_val[p]['question'], get_schema_string(spider_val[p]['db_id'])), spider_val[p]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1218,
     "status": "ok",
     "timestamp": 1747837184171,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "cRxD89qlRNwl",
    "outputId": "cb475911-76cb-4dae-f493-1505dd443b20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_match(generate_sql(spider_val[p]['question'], get_schema_string(spider_val[p]['db_id'])), spider_val[p]['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OngS8i_ZujRJ"
   },
   "source": [
    "# Quick Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6351,
     "status": "ok",
     "timestamp": 1747837415508,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "8W7YhedmX_Bq",
    "outputId": "a2eb989d-8211-41ef-d022-2562480ca854"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold SQL:  SELECT count(*) FROM Documents AS T1 JOIN Templates AS T2 ON T1.Template_ID  =  T2.Template_ID WHERE T2.Template_Type_Code  =  'PPT'\n",
      "Predicted SQL:  SELECT COUNT(*) FROM documents AS T1 JOIN templates AS T2 ON T1.template_id = T2.template_id WHERE T2.template_type_code = \"PPT\"  \n"
     ]
    }
   ],
   "source": [
    "# 1) Size of a 20 % slice (Spider dev has 1 034 samples)\n",
    "num_samples = int(0.20 * len(spider_val))      # → 206\n",
    "\n",
    "sample_val  = spider_val.shuffle(seed=42).select(range(num_samples))\n",
    "\n",
    "example      = sample_val[1]\n",
    "db_id        = example[\"db_id\"]\n",
    "schema_str   = get_schema_string(db_id)\n",
    "predicted_sql= generate_sql(example[\"question\"], schema_str)\n",
    "print(\"Gold SQL: \", example[\"query\"])\n",
    "print(\"Predicted SQL: \", predicted_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1747837429739,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "gO-nfbdBN-jK",
    "outputId": "266adfdf-d51a-4b0a-c790-f64d60ad5daa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'cre_Doc_Template_Mgt',\n",
       " 'query': \"SELECT count(*) FROM Documents AS T1 JOIN Templates AS T2 ON T1.Template_ID  =  T2.Template_ID WHERE T2.Template_Type_Code  =  'PPT'\",\n",
       " 'question': \"How many documents are using the template with type code 'PPT'?\",\n",
       " 'query_toks': ['SELECT',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'FROM',\n",
       "  'Documents',\n",
       "  'AS',\n",
       "  'T1',\n",
       "  'JOIN',\n",
       "  'Templates',\n",
       "  'AS',\n",
       "  'T2',\n",
       "  'ON',\n",
       "  'T1.Template_ID',\n",
       "  '=',\n",
       "  'T2.Template_ID',\n",
       "  'WHERE',\n",
       "  'T2.Template_Type_Code',\n",
       "  '=',\n",
       "  \"'PPT\",\n",
       "  \"'\"],\n",
       " 'query_toks_no_value': ['select',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'from',\n",
       "  'documents',\n",
       "  'as',\n",
       "  't1',\n",
       "  'join',\n",
       "  'templates',\n",
       "  'as',\n",
       "  't2',\n",
       "  'on',\n",
       "  't1',\n",
       "  '.',\n",
       "  'template_id',\n",
       "  '=',\n",
       "  't2',\n",
       "  '.',\n",
       "  'template_id',\n",
       "  'where',\n",
       "  't2',\n",
       "  '.',\n",
       "  'template_type_code',\n",
       "  '=',\n",
       "  'value'],\n",
       " 'question_toks': ['How',\n",
       "  'many',\n",
       "  'documents',\n",
       "  'are',\n",
       "  'using',\n",
       "  'the',\n",
       "  'template',\n",
       "  'with',\n",
       "  'type',\n",
       "  'code',\n",
       "  \"'PPT\",\n",
       "  \"'\",\n",
       "  '?']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQZxXbn8rRi8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1747837470514,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "2lE1BWPeqq8h",
    "outputId": "73c30c55-691f-4899-973d-add375dbf749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "print(exact_match(predicted_sql, example[\"query\"]))\n",
    "print(component_match(predicted_sql, example[\"query\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YF3B0J8ErZ7O"
   },
   "source": [
    "# Simple Random Sampling of 20% of the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 969317,
     "status": "ok",
     "timestamp": 1747839422291,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "W2kGHyD4WCk_",
    "outputId": "df68d845-8089-4063-a84d-f74a5fac0525"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [16:09<00:00,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Exact Match        : 0.009708737864077669\n",
      "Component Match    : 0.9260067581323649\n",
      "Execution Accuracy : 0.470873786407767\n",
      "Avg. Latency  (s)  : 4.621980702470875\n",
      "95% Latency  (s)   : 6.426491680250251\n",
      "GPU Mem Peak       : 18.3346304 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on a fixed 20% Spider dev sample\n",
    "import time, torch, numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from transformers import logging\n",
    "import random\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# 1) Size of a 20 % slice (Spider dev has 1 034 samples)\n",
    "num_samples = int(0.20 * len(spider_val))      # → 206\n",
    "\n",
    "sample_val  = spider_val.shuffle(seed=42).select(range(num_samples))\n",
    "\n",
    "\n",
    "# 2) containers\n",
    "em_scores, cm_scores, ex_scores, times = [], [], [], []\n",
    "\n",
    "# 3) start fresh CUDA-peak tracking\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# 4) main loop\n",
    "for ex in tqdm(sample_val, desc=\"Evaluating\"):\n",
    "    db_id   = ex[\"db_id\"]\n",
    "    schema  = get_schema_string(db_id)\n",
    "\n",
    "    t0      = time.perf_counter()\n",
    "    pred_sql= generate_sql(ex[\"question\"], schema)\n",
    "    times.append(time.perf_counter() - t0)\n",
    "\n",
    "    gold_sql= ex[\"query\"]\n",
    "\n",
    "    # exact + component match\n",
    "    em_scores.append( float(exact_match(pred_sql, gold_sql)) )\n",
    "    cm_scores.append( component_match(pred_sql, gold_sql) )\n",
    "\n",
    "    # execution accuracy\n",
    "    db_path = DB_ROOT / db_id / f\"{db_id}.sqlite\"\n",
    "    gold_res= run_query(gold_sql,  db_path)\n",
    "    pred_res= run_query(pred_sql, db_path)\n",
    "    ex_scores.append( int(gold_res == pred_res) )\n",
    "\n",
    "# 5) aggregate & report\n",
    "print(\"\\n\\nExact Match    :\", np.mean(em_scores))\n",
    "print(\"Component Match    :\", np.mean(cm_scores))\n",
    "print(\"Execution Accuracy :\", np.mean(ex_scores))\n",
    "print(\"Avg. Latency  (s)  :\", np.mean(times))\n",
    "print(\"95% Latency  (s)   :\", np.percentile(times, 95))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Mem Peak       :\", torch.cuda.max_memory_allocated() / 1e9, \"GB\")\n",
    "else:\n",
    "    print(\"GPU Mem Peak       : N/A (CPU)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REqF3Gg7qM-H"
   },
   "source": [
    "# Full Spider Validation data commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1_qEOWXd_kW"
   },
   "outputs": [],
   "source": [
    "# # Evaluation on complete Spider dev sample\n",
    "# import time, torch, numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # 1) Full spider val\n",
    "# sample_val = spider_val\n",
    "\n",
    "# # 2) containers\n",
    "# em_scores, cm_scores, ex_scores, times = [], [], [], []\n",
    "\n",
    "# # 3) start fresh CUDA-peak tracking\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# # 4) main loop\n",
    "# for ex in tqdm(sample_val, desc=\"Evaluating\"):\n",
    "#     db_id   = ex[\"db_id\"]\n",
    "#     schema  = get_schema_string(db_id)\n",
    "\n",
    "#     t0      = time.perf_counter()\n",
    "#     pred_sql= generate_sql(ex[\"question\"], schema)\n",
    "#     times.append(time.perf_counter() - t0)\n",
    "\n",
    "#     gold_sql= ex[\"query\"]\n",
    "\n",
    "#     # exact + component match\n",
    "#     em_scores.append( float(exact_match(pred_sql, gold_sql)) )\n",
    "#     cm_scores.append( component_match(pred_sql, gold_sql) )\n",
    "\n",
    "#     # execution accuracy\n",
    "#     db_path = DB_ROOT / db_id / f\"{db_id}.sqlite\"\n",
    "#     gold_res= run_query(gold_sql,  db_path)\n",
    "#     pred_res= run_query(pred_sql, db_path)\n",
    "#     ex_scores.append( int(gold_res == pred_res) )\n",
    "\n",
    "# # 5) aggregate & report\n",
    "# print(\"\\n\\nExact Match        :\", np.mean(em_scores))\n",
    "# print(\"Component Match    :\", np.mean(cm_scores))\n",
    "# print(\"Execution Accuracy :\", np.mean(ex_scores))\n",
    "# print(\"Avg. Latency  (s)  :\", np.mean(times))\n",
    "# print(\"95% Latency  (s)   :\", np.percentile(times, 95))\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"GPU Mem Peak       :\", torch.cuda.max_memory_allocated() / 1e9, \"GB\")\n",
    "# else:\n",
    "#     print(\"GPU Mem Peak       : N/A (CPU)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DagCg0lJHakG"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade accelerate\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i2h2mmgceV-"
   },
   "source": [
    "# Introducing MiniLM Setence Transformer for Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQ8EMu3cAy94"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1 ▪▪▪  Dynamic-retrieval helper: get_k_shots\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# (Run this exactly once.)  ──────────────────────────────────────────────\n",
    "embedder   = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "spider_train = load_dataset(\"spider\")[\"train\"]          # if not in memory\n",
    "train_emb  = embedder.encode(spider_train[\"question\"], convert_to_tensor=True)\n",
    "\n",
    "def get_k_shots(nl_question: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Return k training examples whose NL questions are semantically closest\n",
    "    to `nl_question`.  Each example is a plain dict containing question,\n",
    "    gold SQL, db_id, and a pre-computed schema string.\n",
    "    \"\"\"\n",
    "    q_emb  = embedder.encode(nl_question, convert_to_tensor=True)\n",
    "    hits   = util.semantic_search(q_emb, train_emb, top_k=k)[0]   # [{corpus_id, score}, …]\n",
    "\n",
    "    demos  = []\n",
    "    for h in hits:\n",
    "        idx = h[\"corpus_id\"]\n",
    "        ex  = spider_train[idx]\n",
    "        demos.append({\n",
    "            \"question\"    : ex[\"question\"],\n",
    "            \"query\"       : ex[\"query\"],\n",
    "            \"db_id\"       : ex[\"db_id\"],\n",
    "            \"schema_str\"  : get_schema_string(ex[\"db_id\"]),\n",
    "        })\n",
    "    return demos\n",
    "\n",
    "\n",
    "# ║ 2 ▪▪▪  (Optional) schema-pruning stub                                ║\n",
    "def prune_schema(query_sql: str, full_schema: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the subset of `full_schema` that is actually referenced in `query_sql`.\n",
    "    For now we simply return full_schema; refine later if you like.\n",
    "    \"\"\"\n",
    "    return full_schema\n",
    "\n",
    "# 3 Build prompt\n",
    "\n",
    "def build_prompt(nl_question, target_schema):\n",
    "\n",
    "    demos = get_k_shots(nl_question, k=5)      # dynamically chosen\n",
    "\n",
    "    parts = [\"### Instruction:\\nReturn ONE SQL query only.\\n\"]\n",
    "\n",
    "    for i, ex in enumerate(demos, 1):\n",
    "\n",
    "        used_schema = prune_schema(ex['query'], ex['schema_str'])  # optional\n",
    "\n",
    "        parts += [\n",
    "\n",
    "            f\"### Example {i} Schema:\\n{used_schema}\",\n",
    "\n",
    "            f\"### Example {i} Question:\\n{ex['question']}\",\n",
    "\n",
    "            f\"### Example {i} SQL:\\n{ex['query'].strip()}\",\n",
    "\n",
    "            \"### End\\n\"\n",
    "\n",
    "        ]\n",
    "\n",
    "    parts += [f\"### Database Schema:\\n{target_schema}\",\n",
    "\n",
    "              f\"### Question:\\n{nl_question}\",\n",
    "\n",
    "              \"### SQL:\\n\"]\n",
    "\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "# 3  Fix the small typo in generate_sql\n",
    "import re\n",
    "\n",
    "def generate_sql(nl_question: str, schema: str) -> str:\n",
    "    prompt     = build_prompt(nl_question, schema)\n",
    "    inputs     = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_len  = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    gen_text = tokenizer.decode(out[0, input_len:], skip_special_tokens=True)\n",
    "\n",
    "    # keep everything before the first sentinel (`###`, blank line, or back-ticks)\n",
    "    sql = re.split(r\"(###|\\n\\s*\\n|```)\", gen_text, maxsplit=1)[0]\n",
    "    sql = sql.split(\";\", 1)[0]                 # Spider queries are single-stmt\n",
    "    return sql.strip()                         # ← RETURN VARIABLE WAS “SQL” BEFORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awW39G3tS-2U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F99BbLwRBIca"
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "# def generate_sql(nl_question: str, schema: str) -> str:\n",
    "#     prompt     = build_prompt(nl_question, schema)\n",
    "#     inputs     = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     input_len  = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "#     model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#     out        = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=128,\n",
    "#         num_beams=5,\n",
    "#         early_stopping=True,\n",
    "#         do_sample=False,\n",
    "#     )\n",
    "\n",
    "#     gen_tokens = out[0, input_len:]\n",
    "#     sql        = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "#     # trim markdown fences etc.\n",
    "#     #sql = sql.split(\"```\")[-1].strip()\n",
    "#     sql = sql.split(\";\")[0].replace(\"\\n\", \" \").strip()\n",
    "#     #return sql\n",
    "\n",
    "#     # keep everything *before* first sentinel\n",
    "#     sql = re.split(r\"(###|\\n\\n)\", sql, maxsplit=1)[0]\n",
    "#     # remove prefix for llama3.1\n",
    "#     prefix = \"```sql \"\n",
    "#     if sql.startswith(prefix):\n",
    "#       sql = sql[len(prefix):]\n",
    "#     else:\n",
    "#       sql = sql\n",
    "\n",
    "\n",
    "#     return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1200964,
     "status": "ok",
     "timestamp": 1747859293487,
     "user": {
      "displayName": "John Eze",
      "userId": "00129604659434633188"
     },
     "user_tz": -120
    },
    "id": "l9Kj7mEhBkCQ",
    "outputId": "8c9535b9-5e07-4942-bb84-d11bb5601a1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [20:00<00:00,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Exact Match    : 0.1553398058252427\n",
      "Component Match    : 0.9015262503611605\n",
      "Execution Accuracy : 0.5631067961165048\n",
      "Avg. Latency  (s)  : 5.71617229285437\n",
      "95% Latency  (s)   : 6.922948900249821\n",
      "GPU Mem Peak       : 21.245072896 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on a fixed 20% Spider dev sample\n",
    "import time, torch, numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from transformers import logging\n",
    "import random\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# 1) Size of a 20 % slice (Spider dev has 1 034 samples)\n",
    "num_samples = int(0.20 * len(spider_val))      # → 206\n",
    "\n",
    "sample_val  = spider_val.shuffle(seed=42).select(range(num_samples))\n",
    "\n",
    "\n",
    "# 2) containers\n",
    "em_scores, cm_scores, ex_scores, times = [], [], [], []\n",
    "\n",
    "# 3) start fresh CUDA-peak tracking\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# 4) main loop\n",
    "for ex in tqdm(sample_val, desc=\"Evaluating\"):\n",
    "    db_id   = ex[\"db_id\"]\n",
    "    schema  = get_schema_string(db_id)\n",
    "\n",
    "    t0      = time.perf_counter()\n",
    "    pred_sql= generate_sql(ex[\"question\"], schema)\n",
    "    times.append(time.perf_counter() - t0)\n",
    "\n",
    "    gold_sql= ex[\"query\"]\n",
    "\n",
    "    # exact + component match\n",
    "    em_scores.append( float(exact_match(pred_sql, gold_sql)) )\n",
    "    cm_scores.append( component_match(pred_sql, gold_sql) )\n",
    "\n",
    "    # execution accuracy\n",
    "    db_path = DB_ROOT / db_id / f\"{db_id}.sqlite\"\n",
    "    gold_res= run_query(gold_sql,  db_path)\n",
    "    pred_res= run_query(pred_sql, db_path)\n",
    "    ex_scores.append( int(gold_res == pred_res) )\n",
    "\n",
    "# 5) aggregate & report\n",
    "print(\"\\n\\nExact Match    :\", np.mean(em_scores))\n",
    "print(\"Component Match    :\", np.mean(cm_scores))\n",
    "print(\"Execution Accuracy :\", np.mean(ex_scores))\n",
    "print(\"Avg. Latency  (s)  :\", np.mean(times))\n",
    "print(\"95% Latency  (s)   :\", np.percentile(times, 95))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Mem Peak       :\", torch.cuda.max_memory_allocated() / 1e9, \"GB\")\n",
    "else:\n",
    "    print(\"GPU Mem Peak       : N/A (CPU)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4E5h5XABDkSx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOCDCiSK/NfcbjRY7jo9Z8t",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1hMkBDzyuaQ6O7jmbMZ0ujbp7E175cDUb",
     "timestamp": 1747794022424
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1e43713ebfa2455d867b1487875ab5ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2716660f92254ef6abe90b7cfe8c4157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea35617346ab41309034158a72463f73",
      "placeholder": "​",
      "style": "IPY_MODEL_f6438de878544783934be004023e10d1",
      "value": " 4/4 [00:05&lt;00:00,  1.13s/it]"
     }
    },
    "27b965a7348b49a8b8ad7273b3d1f7d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9687eabbe5514da8bc475fa5ed4e1d0e",
       "IPY_MODEL_f2d210323c5e44fb8d3fb44f2c44afe5",
       "IPY_MODEL_2716660f92254ef6abe90b7cfe8c4157"
      ],
      "layout": "IPY_MODEL_1e43713ebfa2455d867b1487875ab5ff"
     }
    },
    "30d169b1e3e144c39b3b453c46323e3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37c40d810f9142949f7cee8d14747dce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9687eabbe5514da8bc475fa5ed4e1d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37c40d810f9142949f7cee8d14747dce",
      "placeholder": "​",
      "style": "IPY_MODEL_aa7e83b290484038b31f6accf73ec714",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "aa7e83b290484038b31f6accf73ec714": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db8757e19b0249e1ad5279b0e2ccb254": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea35617346ab41309034158a72463f73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2d210323c5e44fb8d3fb44f2c44afe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db8757e19b0249e1ad5279b0e2ccb254",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_30d169b1e3e144c39b3b453c46323e3d",
      "value": 4
     }
    },
    "f6438de878544783934be004023e10d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
